<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Haolin Yang*</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="">Feilong Tang*</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <a href="">Ming Hu</a><sup>1,2,3</sup>,
            </span>
            <span class="author-block">
              <a href="">Yulong Li</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="">Junjie Guo</a><sup>4</sup>,
            </span>
             <span class="author-block">
              <a href="">Yexin Liu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="">Zelin Peng</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="">Junjun He</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">Zongyuan Ge†</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="">Imran Razzak†</a><sup>1</sup>
<!--             </span>
            <span class="author-block">
              <a href="">Sijin Zhou</a><sup>1</sup>
            </span>
                <span class="author-block">
              <a href=""> Wenxue Li</a><sup>1</sup>
            </span>
                <span class="author-block">
              <a href="">Yulong Li</a><sup>2</sup>
            </span>
                <span class="author-block">
              <a href="">Wenxuan Song</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="">Shiyan Su</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="">Wei Feng</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="">Jionglong Su</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="">Minquan Lin</a><sup>6</sup>
            </span>
            <span class="author-block">
              <a href="">Yifan Peng</a><sup>7</sup>
            </span>
            <span class="author-block">
              <a href="">Xuelian Cheng</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="">Imran Razzak†</a><sup>8</sup>
            </span>
              <span class="author-block">
              <a href="">Zongyuan Ge†</a><sup>1</sup> -->
            </span>
            
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>MBZUAI</span>
            <span class="author-block"><sup>2</sup>Monash University</span>
            <span class="author-block"><sup>3</sup>Shanghai AI Lab</span>
            <span class="author-block"><sup>4</sup>Nanjing Universityty</span>
            <span class="author-block"><sup>5</sup>HKUST</span>
            <span class="author-block"><sup>6</sup>Shanghai Jiaotong University</span>
<!--             <span class="author-block"><sup>2</sup>Cornell University</span>
            <span class="author-block"><sup>2</sup>University of New South Wales</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 60%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/intro.png" 
      alt="image" 
      style="width: 60%; height: auto; margin-left: 25%;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
      (a): Attention Collapse in MLLMs: Outlier tokens from different modalities are assigned disproportionately high attention scores, hindering interaction between relevant tokens. (b): Positional Information Decay: As text generation progresses, attention to visual information gradually diminishes. (c): Our FarSight, as a plug-in, mitigates these issues by effectively reducing attention interference from outlier tokens and improving response accuracy.
    </figcaption>
  </figure>
</section> -->

  
 <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Video diffusion models (VDMs) facilitate the generation of high-quality videos, with current research predominantly concentrated on scaling efforts during training through improvements in data quality, computational resources, and model complexity. However, inference-time scaling has received less attention, with most approaches restricting models to a single generation attempt. Recent studies have uncovered the existence of “golden noise" that can enhance video quality during generation. Building on this, we find that guiding the scaling inference-time search of VDMs to identify better noise candidates not only evaluates the quality of the frames generated in the current step but also preserves the high-level object features by referencing the anchor frame from previous multi-chunks, thereby delivering long-term value. Our analysis reveals that diffusion models inherently possess flexible adjustments of computation by varying denoising steps, and even a one-step denoising approach, when guided by a reward signal, yields significant long-term benefits. Based on the observation, we propose ScalingNoise, a plug-and-play inference-time search strategy that identifies golden initial noise for the diffusion sampling process to improve global content consistency and visual diversity. Specifically, we perform one-step denoising to convert initial noise into a clip and subsequently evaluate its long-term value, leveraging a reward model anchored by previously generated content. Moreover, to preserve diversity, we sample candidates from a tilted noise distribution that up-weights promising noises. In this way, ScalingNoise significantly reduces noise-induced errors, ensuring more coherent and spatiotemporally consistent video generation. Extensive experiments on benchmark datasets demonstrate that the proposed ScalingNoise effectively improves both content fidelity and subject consistency for resource-constrained long video generation.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Methodology</h2>
      </div>
      </div>

<!--    <section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 80%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/method.png" 
      alt="image" 
      style="width: 100%; height: auto;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
      The scheme of the proposed FarSight strategy, which integrates with the softmax operation, replaces the traditional causal mask. Specifically, the attention score matrix ω is cleared of attention values in the upper triangular part, then register-attention scores are added using the matrix P, followed by the softmax computation. P has a linear decay in the upper triangular part and zeros in the lower triangular part. After the softmax operation, the remaining attention probabilities in the upper triangular part are cleared to ensure the causal decoding property is preserved.

    </figcaption>
  </figure>
</section> -->

  <section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 80%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/method.png" 
      alt="image" 
      style="width: 100%; height: auto;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
      The overview of proposed method, ScalingNoise, which improves long video generation through inference-time search, replaces the training-time scaling.
Specailly, our strategy mitigates this by conducting a tailored step-by-step beam search for suitable initial noise, guided by a reward model that incorporates an anchor frame to ensure a long-term signal.
At each step, we perform one-step denoising on candidate noises to obtain a clearer clip for evaluation; the reward model then predicts the long-term value of each candidate, helping avoid noise that could introduce future inconsistencies.
    </figcaption>
  </figure>
</section>



   <div class="columns is-centered has-text-centered" style="margin-top: 0rem;">
  <div class="column is-four-fifths">
    <h2 class="title is-3">FIFO-Diffusion+ScaleNoise</h2>
  </div>
</div>
<div class="columns is-centered has-text-centered" style="margin-top: 1rem;">
  <div class="column is-four-fifths" style="display: flex; justify-content: space-around; flex-wrap: wrap;">
    <div style="flex: 0 1 48%; margin: 1rem 0;">
      <video controls>
        <source src="static/videos/Impressionist style, a yellow rubber duck floating on the wave on the sunset, 4k resolution._0.mp4" type="video/mp4">
      </video>
    </div>
    <div style="flex: 0 1 48%; margin: 1rem 0;">
      <video controls>
        <source src="static/videos/A cute raccoon playing guitar in a boat on the ocean, 4k resolution._0.mp4" type="video/mp4">
      </video>
    </div>
  </div>

<div class="columns is-centered has-text-centered" style="margin-top: 1rem;">
  <div class="column is-four-fifths" style="display: flex; justify-content: space-around; flex-wrap: wrap;">
    <div style="flex: 0 1 48%; margin: 1rem 0;">
      <video controls>
        <source src="static/videos/A cozy, low-poly cabin in the woods surrounded by tall pine trees, with a warm light glowing from th_0.mp4" type="video/mp4">
      </video>
    </div>
    <div style="flex: 0 1 48%; margin: 1rem 0;">
      <video controls>
        <source src="static/videos/A tranquil, low-poly mountain village at dawn, with smoke rising from chimneys and birds flying over_0.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</div>

  <div class="columns is-centered has-text-centered" style="margin-top: 1rem;">
  <div class="column is-four-fifths" style="display: flex; justify-content: space-around; flex-wrap: wrap;">
    <div style="flex: 0 1 48%; margin: 1rem 0;">
      <video controls>
        <source src="static/videos/Cinematic closeup and detailed portrait of a reindeer in a snowy forest at sunset. The lighting is c_1.mp4" type="video/mp4">
      </video>
    </div>
    <div style="flex: 0 1 48%; margin: 1rem 0;">
      <video controls>
        <source src="static/videos/A spectacular fireworks display over Sydney Harbour, 4K, high resolution._0.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</div>
   static/videos/A cozy, low-poly cabin in the woods surrounded by tall pine trees, with a warm light glowing from th_0.mp4
<!-- <section class="section" style="margin-top: -2.6rem;">
  <figure class="image is-centered" style="width: 120%; margin: 0 auto 1rem; text-align: center;">
    <img 
      src="./static/images/exp.png" 
      alt="image" 
      style="width: 120%; height: auto;" 
      loading="lazy"
    />
    <figcaption style="text-align: center;">
     Qualitative Visualization of FarSight in Image Understanding Task on LLaVA-1.5. (a) Comparison of the average attention allocation to images during text generation among Base (Vanilla MLLMs), EDVT and our FarSight; (b) Visual attention decay across different methods within the generation of 60 text tokens; (c) FarSight's attention distribution on images under varying decay rat $\sigma$. More detailed visualizations of images and videos are provided in Appendix F.
    </figcaption>
  </figure>
</section> -->

<section class="section" id="BibTeX">
  <div class="container content" style="max-width: 800px; margin: 0 auto;">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      
    </code></pre>
  </div>
</section>

</body>
</html>
